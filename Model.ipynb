{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":95687,"databundleVersionId":11957320,"sourceType":"competition"},{"sourceId":11518545,"sourceType":"datasetVersion","datasetId":7223876},{"sourceId":11518686,"sourceType":"datasetVersion","datasetId":7223989}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install faiss-cpu --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:01:49.688416Z","iopub.execute_input":"2025-04-23T12:01:49.688691Z","iopub.status.idle":"2025-04-23T12:01:55.649644Z","shell.execute_reply.started":"2025-04-23T12:01:49.688671Z","shell.execute_reply":"2025-04-23T12:01:55.648952Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, top_k_accuracy_score\nfrom sklearn.preprocessing import normalize\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import resnet50\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport joblib\nimport faiss\nfrom collections import defaultdict\nimport multiprocessing\nimport gc\n\n# Configuration\nNUM_CLASSES = 500  # Assuming 500 classes; adjust if different\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nCPU_COUNT = multiprocessing.cpu_count()\nprint(f\"Using {CPU_COUNT} CPU cores\")\n\n\n#X_train_raw, y_train = joblib.load(\"/kaggle/input/fulldata/train_features (1).pkl\")\n\n#print(f\"Reloaded features: Train {X_train_raw.shape}, Val \")\n\n\nprint(\"Cell 1 done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:01:55.651103Z","iopub.execute_input":"2025-04-23T12:01:55.651391Z","iopub.status.idle":"2025-04-23T12:02:07.910324Z","shell.execute_reply.started":"2025-04-23T12:01:55.651362Z","shell.execute_reply":"2025-04-23T12:02:07.909675Z"}},"outputs":[{"name":"stdout","text":"Using 4 CPU cores\nCell 1 done\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import random\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport joblib\n\n# Load pre-extracted features and labels\nx_train_raw, Y_train = joblib.load(\"/kaggle/input/fulldata/train_features (1).pkl\")\nprint(type(x_train_raw))\nprint(type(Y_train))\n\n# Group all feature vectors per class\nclass_to_indices = defaultdict(list)\nfor idx, label in enumerate(Y_train):\n    class_to_indices[label].append(idx)\n\n# Sample 1–2 random items per class\nselected_indices = []\nfor label, indices in class_to_indices.items():\n    num_samples = min(2, len(indices))  # in case a class has only 1 image\n    selected_indices.extend(random.sample(indices, num_samples))\n\n# Select the features and labels\nX_train_raw =np.array([x_train_raw[i] for i in selected_indices])\ny_train = [Y_train[i] for i in selected_indices]\n\nX_test_raw , img_names = joblib.load('/kaggle/input/testingfeatures/test_features.pkl')\nprint(f\"Loaded test data :{len(X_test_raw)}\")\nprint(f\"Selected {len(y_train)} samples from {len(class_to_indices)} classes.\")\nprint(len(X_train_raw))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:36:14.746444Z","iopub.execute_input":"2025-04-23T12:36:14.746718Z","iopub.status.idle":"2025-04-23T12:36:18.245286Z","shell.execute_reply.started":"2025-04-23T12:36:14.746698Z","shell.execute_reply":"2025-04-23T12:36:18.244438Z"}},"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n<class 'list'>\nLoaded test data :113592\nSelected 227184 samples from 113592 classes.\n227184\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nimport gc\n\n# ==== Step 1: Scale the data ====\nscaler_pca = StandardScaler().fit(X_train_raw)\nscaler_ica = StandardScaler().fit(X_train_raw)\n\nXp_train = scaler_pca.transform(X_train_raw)\nXi_train = scaler_ica.transform(X_train_raw)\nXp_test = scaler_pca.transform(X_test_raw)\nXi_test = scaler_ica.transform(X_test_raw)\n\ndel scaler_pca, scaler_ica\ngc.collect()\n\n# ==== Step 2: PCA ====\npca = PCA(n_components=0.95, svd_solver='full', random_state=SEED)\nXp_train_pca = pca.fit_transform(Xp_train)\nXp_test_pca = pca.transform(Xp_test)\nn_features = Xp_train_pca.shape[1]\n\ndel Xp_train, Xp_test, pca\ngc.collect()\n\n# ==== Step 3: LDA ====\nn_classes = len(np.unique(y_train))\nn_lda_components = min(n_features, n_classes - 1)\n\nlda = LDA(n_components=n_lda_components)\nX_lda_train = lda.fit_transform(Xp_train_pca, y_train)\nX_lda_test = lda.transform(Xp_test_pca)\n\ndel lda\ngc.collect()\n\n# ==== Step 4: ICA ====\nn_ica_components = min(200, X_train_raw.shape[0])\nica = FastICA(n_components=n_ica_components, max_iter=200, random_state=SEED)\n\nXi_train_ica = ica.fit_transform(Xi_train)\nXi_test_ica = ica.transform(Xi_test)\n\ndel Xi_train, Xi_test, ica\ngc.collect()\n\n# ==== Step 5: Fuse features ====\nX_fused_train = np.hstack([Xp_train_pca, X_lda_train, Xi_train_ica])\nX_fused_test = np.hstack([Xp_test_pca, X_lda_test, Xi_test_ica])\n\ndel Xp_train_pca, Xp_test_pca, X_lda_train, X_lda_test, Xi_train_ica, Xi_test_ica\ngc.collect()\n\n# ==== Step 6: Normalize ====\nX_fused_train = normalize(X_fused_train, axis=1)\nX_fused_test = normalize(X_fused_test, axis=1)\n\njoblib.dump((X_fused_train,y_train),\"fused_train.pkl\")\njoblib.dump(X_fused_test,\"fused_test.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:36:24.093100Z","iopub.execute_input":"2025-04-23T12:36:24.094007Z","iopub.status.idle":"2025-04-23T12:44:10.722743Z","shell.execute_reply.started":"2025-04-23T12:36:24.093975Z","shell.execute_reply":"2025-04-23T12:44:10.721879Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"['fused_test.pkl']"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"X_fused_train, y_train =  joblib.load(\"fused_train_features.pkl\")\nX_train_raw, y_train =  joblib.load(\"raw_train_features.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\nimport faiss\nfrom tqdm import tqdm\n'''\n# NEEDS : X_FUSED_TRAIN , \n# NEEDS : Y_TRAIN\n#\n#\n'''\n# ── Assumptions: \n\n# ── 1) Build FAISS index on your TRAIN fused features ─────────────────────────\nd = X_fused_train.shape[1]\nindex = faiss.IndexFlatL2(d)\nindex.add(X_fused_train.astype('float32'))\n\n# Keep the parallel label list\nfaiss_labels = y_train\n\nprint(\"FAISS index built and training labels stored.\")\n\ntest_paths = img_names\n\n\n# ── 2) Query each TEST embedding and collect top‑3 predictions ────────────────\nk = 3\nresults = []\n\nfor i, img_path in tqdm(enumerate(test_paths), total=len(test_paths), desc=\"Searching test set\"):\n    # 2a) grab the i-th test embedding\n    emb = X_fused_test[i].astype('float32').reshape(1, -1)\n    # 2b) search top‑k\n    D, I = index.search(emb, k)\n    # 2c) map back to your original labels\n    top3 = [faiss_labels[idx] for idx in I[0]]\n    # 2d) record: imagename + 3 preds\n    img_name = os.path.basename(img_path)\n    results.append([f\"test/{img_name}\", *top3])\n    \n\n# ── 3) Dump to CSV ────────────────────────────────────────────────────────────\nout_csv = \"test_predictions.csv\"\nwith open(out_csv, \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"filename\", \"label_1\", \"label_2\", \"label_3\"])\n    writer.writerows(results)\n\nprint(f\"Wrote {out_csv}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:56:45.706528Z","iopub.execute_input":"2025-04-23T12:56:45.706802Z"}},"outputs":[{"name":"stdout","text":"FAISS index built and training labels stored.\n","output_type":"stream"},{"name":"stderr","text":"Searching test set:   2%|▏         | 2516/113592 [03:32<2:35:19, 11.92it/s]","output_type":"stream"}],"execution_count":null}]}